{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e9f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88793506",
   "metadata": {},
   "source": [
    "### Back propagation formula\n",
    "The four equations for doing back propagation:\n",
    "$$\\begin{eqnarray}\\delta^L&=&\\nabla_aC\\odot\\sigma'(z^L) \\\\\n",
    "\\delta^l&=&((w^{l+1})^T\\delta^{l+1})\\odot\\sigma'(z^l) \\\\ \\frac{\\partial C}{\\partial b_j^l}&=&\\delta_j^l \\\\\n",
    "\\frac{\\partial C}{\\partial w_{jk}^l}&=&a_k^{l-1}\\delta_j^l\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "where $C$ is the cost function, ${a^l_k}$ is the activation of the ${k_{th}}$ neuron in the ${l_{th}}$ layer, ${w^l}$ and ${b^l}$ are the weights and the bias vector connecting to the ${l_{th}}$ layer, and $z^l$ is the weighted input. Thus ${a^l}$ can be written as $\\sigma(z^l)$ where $\\sigma$ is the activation function. ${\\delta^l}$ as the vector error in layer $l$ and ${\\delta^L}$ as the vector error for the output layer.  \n",
    "Credit: [Neural Networks and Deep Learning, Ch. 2](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4b8ea",
   "metadata": {},
   "source": [
    "#### Be mindful of element-wise product $\\odot$ vs. dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d362e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  4,  9, 16, 25, 36, 49])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element wise product \n",
    "a = np.arange(8)\n",
    "print(a)\n",
    "a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0370993c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "a.dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b0902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self,architecture,learning_rate=0.01,activation=lambda x:x):\n",
    "        '''This is a fully connected NN. The architecture is a list, \n",
    "        with each element specifying the number of nodes in each layer'''\n",
    "        self.arch=architecture\n",
    "        self.activation=activation # your activation function\n",
    "        self.lr=learning_rate\n",
    "        self.initialized=False\n",
    "        # add more attributes if needed\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        for n in range(sum(self.arch[:2])*2):\n",
    "            # use random number to initial weights here\n",
    "            self.weights.append(np.random.random())\n",
    "            if(n % 4 == 0):\n",
    "                self.biases.append(np.random.random())\n",
    "        self.weights_12 = np.array(self.weights[:12]).reshape((6,2))\n",
    "        self.weights_23 = np.array(self.weights[12:]).reshape((2,2))\n",
    "        self.biases = np.array(self.biases).reshape((2,2))\n",
    "        self.initialized=True\n",
    "        return self.weights,self.biases\n",
    "        \n",
    "    def feed_forward(self,x):\n",
    "        if self.initialized:\n",
    "            # define necessary variables\n",
    "            # you may want to keep a record of values for each layer\n",
    "            # loop over forward steps\n",
    "            self.inputs = x\n",
    "            inputs = np.array(x)\n",
    "            self.z1 = inputs.dot(self.weights_12) + self.biases[0,:]\n",
    "            self.act_2nd_layer = self.activation(self.z1)\n",
    "            self.z2 = self.act_2nd_layer.dot(self.weights_23) + self.biases[1,:]\n",
    "            self.act_output = self.activation(self.z2)\n",
    "            return self.act_output\n",
    "        else:\n",
    "            print(\"Please initialize the weights first!\")\n",
    "        \n",
    "    \n",
    "    def back_prop(self):\n",
    "        # Using back-propagation, update weights with a learning parameter\n",
    "        dc_outerb = self.error_output\n",
    "        dc_hiddenb = self.error_hidden\n",
    "        dc_wouter = np.vstack((self.act_2nd_layer, self.act_2nd_layer)).T * self.error_output\n",
    "        dc_whidden = np.vstack((self.inputs, self.inputs)).T * self.error_hidden\n",
    "        self.biases[1,:] = self.biases[1,:] - ((self.lr) * dc_outerb)\n",
    "        self.biases[0,:] = self.biases[0, :] - ((self.lr) * dc_hiddenb)\n",
    "        self.weights_23 = self.weights_23 - ((self.lr) * dc_wouter)\n",
    "        self.weights_12 = self.weights_12 - ((self.lr) * dc_whidden)\n",
    "        return self.biases,self.weights_12, self.weights_23\n",
    "            \n",
    "    def fit(self,x,y,activation_grad):\n",
    "        # train with (x, y) using feed-forward back-propagation\n",
    "        # activation_grad: gradient function for your activation function\n",
    "        pred_output = self.predict(x)\n",
    "        print('Predicted Ouput')\n",
    "        print(pred_output)\n",
    "        lay_errors = self.layer_errors(activation_grad)\n",
    "        print('Output & Hidden Layer Errors in That Order')\n",
    "        print(lay_errors)\n",
    "        new_weights = self.back_prop()\n",
    "        print('New Biases, New Weights connecting Input to Hidden, New Weights connecting Hidden to Output')\n",
    "        print(new_weights)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return self.feed_forward(x)\n",
    "    \n",
    "    def layer_errors(self, activation_grad):\n",
    "        grad_cost = np.array([(-1 - self.act_output[0]), (-1 - self.act_output[1])])\n",
    "        self.error_output = grad_cost * activation_grad(self.z2)\n",
    "        self.error_hidden = (self.weights_23.T.dot(self.error_output)) * (activation_grad(self.z1))\n",
    "        return self.error_output, self.error_hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aae1938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Ouput\n",
      "[-0.40806365 -0.73304806]\n",
      "Output & Hidden Layer Errors in That Order\n",
      "(array([-0.49336951, -0.12350279]), array([-0.25441521, -0.30135118]))\n",
      "New Biases, New Weights connecting Input to Hidden, New Weights connecting Hidden to Output\n",
      "(array([[0.61982275, 0.07659692],\n",
      "       [0.65819187, 0.04793348]]), array([[0.54903414, 0.0373644 ],\n",
      "       [0.89000455, 0.40458017],\n",
      "       [0.49118796, 0.72942045],\n",
      "       [0.38447091, 0.31411679],\n",
      "       [0.01492478, 0.74039311],\n",
      "       [0.96016532, 0.7592653 ]]), array([[0.78895717, 0.67050514],\n",
      "       [0.93149726, 0.89964125]]))\n"
     ]
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "nn=NN([6,2,2], activation=tanh)\n",
    "nn.init_weights()\n",
    "nn.predict([-1,1,-1,-1,1,-1])\n",
    "nn.fit([-1,1,-1,-1,1,-1], [-1,-1], tanh_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532ca2e",
   "metadata": {},
   "source": [
    "## 2a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c7ed78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2132003908166632,\n",
       " 0.23435013712904007,\n",
       " 0.10002727308763193,\n",
       " 0.16498758438894945,\n",
       " 0.3230570998725264,\n",
       " 0.8515414593015405,\n",
       " 0.14317349923896794,\n",
       " 0.5679163559208277,\n",
       " 0.49357463484088704,\n",
       " 0.011513949360452913,\n",
       " 0.6595961610778186,\n",
       " 0.37019428703675916,\n",
       " 0.44767204931209714,\n",
       " 0.8642870386850569,\n",
       " 0.3354986300941968,\n",
       " 0.019868237932705868]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn=NN([6,2,2], activation=tanh)\n",
    "weight, bias = nn.init_weights()\n",
    "print(len(weight))\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53b210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74384522, 0.4832384 ],\n",
       "       [0.21670742, 0.52172496]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0babe",
   "metadata": {},
   "source": [
    "## 2b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae62322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07819903,  0.4644163 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = nn.predict([-1,1,-1,-1,1,-1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96758f33",
   "metadata": {},
   "source": [
    "I am getting different outputs for the same input of [-1,1,-1,-1,1,-1] and I am assuming that is due to the use of random weights and random biases. One of the outputs that I got was [-0.22, -0.44] and given that both of the values of the output is negative, I am designating its corresponding secondary structure as a coil. I chose coil since its predicted output is (-1,-1), so both values of the output are negative just like the example output that I obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb850e2",
   "metadata": {},
   "source": [
    "## 2c:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf756e4",
   "metadata": {},
   "source": [
    "$C = 1/2 * ∑_j(y_j−a^L_j)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec0603",
   "metadata": {},
   "source": [
    "$ ∂C/∂a^L_j=(a^L_j−y_j) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b095eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.91616407, -1.14856733]), array([-0.79548183, -0.18753859]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layer_errors(tanh_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128ae85",
   "metadata": {},
   "source": [
    "## 2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12f97be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.75180004, 0.48511378],\n",
       "        [0.22586906, 0.53321063]]),\n",
       " array([[0.20524557, 0.23247475],\n",
       "        [0.10798209, 0.16686297],\n",
       "        [0.31510228, 0.84966607],\n",
       "        [0.13521868, 0.56604097],\n",
       "        [0.50152945, 0.01338934],\n",
       "        [0.65164134, 0.3683189 ]]),\n",
       " array([[0.44765757, 0.86426889],\n",
       "        [0.32746041, 0.00979096]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function updates weights & biases\n",
    "nn.back_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084b3f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20524557, 0.23247475],\n",
       "       [0.10798209, 0.16686297],\n",
       "       [0.31510228, 0.84966607],\n",
       "       [0.13521868, 0.56604097],\n",
       "       [0.50152945, 0.01338934],\n",
       "       [0.65164134, 0.3683189 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearly new weights were successfully updated for connection of input to hidden layer\n",
    "nn.weights_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e07ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44765757, 0.86426889],\n",
       "       [0.32746041, 0.00979096]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearly new weights were successfully updated for connection of hidden to output\n",
    "nn.weights_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85abd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75180004, 0.48511378],\n",
       "       [0.22586906, 0.53321063]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearly new biases were successfully updated as well\n",
    "nn.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d45dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Ouput\n",
      "[-0.03622421  0.51636087]\n",
      "Output & Hidden Layer Errors in That Order\n",
      "(array([-0.96251113, -1.11205577]), array([-0.79270695, -0.19852233]))\n",
      "New Biases, New Weights connecting Input to Hidden, New Weights connecting Hidden to Output\n",
      "(array([[0.75972711, 0.48709901],\n",
      "       [0.23549417, 0.54433119]]), array([[0.1973185 , 0.23048953],\n",
      "       [0.11590916, 0.16884819],\n",
      "       [0.30717521, 0.84768085],\n",
      "       [0.12729161, 0.56405575],\n",
      "       [0.50945652, 0.01537456],\n",
      "       [0.64371427, 0.36633368]]), array([[4.48177820e-01, 8.64869968e-01],\n",
      "       [3.19044973e-01, 6.80260669e-05]]))\n"
     ]
    }
   ],
   "source": [
    "# new Run\n",
    "nn.fit([-1,1,-1,-1,1,-1], [-1,-1], tanh_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
