{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10044e65",
   "metadata": {},
   "source": [
    "## 1a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95741fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1739d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 32, 32)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 32, 32)\n",
      "Y_test:  (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_X, train_y), (test_X, test_y) = pickle.load(open(\"mnist.pkl\", \"rb\"))\n",
    " \n",
    "#shape of dataset\n",
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))\n",
    " \n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "for i in range(9):  \n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(train_X[i], cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db3a1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(in_channels= 1, out_channels= 6,kernel_size = 5, stride = 1, padding= 0),\n",
    "                                   nn.Conv2d(in_channels= 6, out_channels= 12,kernel_size = 5, stride = 1, padding= 0)])\n",
    "        self.fc = nn.ModuleList([nn.Linear(300, 300), nn.Linear(300, 10) ])\n",
    "        self.bn = [nn.BatchNorm2d(6),nn.BatchNorm2d(12)]\n",
    "        self.pooling = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(2):\n",
    "            x = self.pooling(self.activation(self.bn[i](self.conv[i](x))))\n",
    "        x = nn.Flatten() (x)\n",
    "        res2 = x\n",
    "        y = self.activation(self.fc[0] (x))\n",
    "        y = y + res2\n",
    "        y = self.fc[1](y)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b602d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-1                       [-1, 6, 28, 28]           156\n",
      "├─ReLU: 1-1                              [-1, 6, 28, 28]           --\n",
      "├─MaxPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-2                       [-1, 12, 10, 10]          1,812\n",
      "├─ReLU: 1-3                              [-1, 12, 10, 10]          --\n",
      "├─MaxPool2d: 1-4                         [-1, 12, 5, 5]            --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-3                       [-1, 300]                 90,300\n",
      "├─ReLU: 1-5                              [-1, 300]                 --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-4                       [-1, 10]                  3,010\n",
      "==========================================================================================\n",
      "Total params: 95,278\n",
      "Trainable params: 95,278\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.39\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 0.41\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = CNN()\n",
    "# Input is the (number of channels, image height, image width) if input is 2d, essentially the 2 dimensions of it\n",
    "s = summary(model, (1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d6c85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func:%r  took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a4c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_chunks(complete_list, chunk_size=None, num_chunks=None):\n",
    "    '''\n",
    "    Cut a list into multiple chunks, each having chunk_size (the last chunk might be less than chunk_size) or having a total of num_chunk chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    if num_chunks is None:\n",
    "        num_chunks = math.ceil(len(complete_list) / chunk_size)\n",
    "    elif chunk_size is None:\n",
    "        chunk_size = math.ceil(len(complete_list) / num_chunks)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(complete_list[i * chunk_size: (i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, \n",
    "                 input_transform=lambda x: x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = Adam(model.parameters(), learning_rate)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "\n",
    "    @timing\n",
    "    def train(self, inputs, outputs, val_inputs, val_outputs,early_stop=False,l2=False,silent=False, tens_logger = None):\n",
    "        \"\"\" train self.model with specified arguments\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_inputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \n",
    "        @return\n",
    "        a dictionary of arrays with train and val losses and accuracies\n",
    "        \"\"\"\n",
    "        ### convert data to tensor of correct shape and type here ###\n",
    "        # Conversion of input arrays to correct shape\n",
    "        train_input = self.input_transform(inputs)\n",
    "        val_inputs = self.input_transform(val_inputs)\n",
    "        \n",
    "        # Conversion of arrays to tensors\n",
    "        inputs = torch.tensor(train_input, dtype= torch.float)\n",
    "        outputs = torch.tensor(outputs, dtype = torch.int64)\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            batch_indices = list(range(inputs.shape[0]))\n",
    "            random.shuffle(batch_indices)\n",
    "            batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for batch in batch_indices:\n",
    "                batch_importance = len(batch) / len(outputs)\n",
    "                batch_input = inputs[batch]\n",
    "                batch_output = outputs[batch]\n",
    "                ### make prediction and compute loss with loss function of your choice on this batch ###\n",
    "                batch_predictions = self.model(batch_input)\n",
    "                loss = nn.CrossEntropyLoss()(batch_predictions, batch_output)\n",
    "                if l2:\n",
    "                    ### Compute the loss with L2 regularization ###\n",
    "                    # Square and sum each entry in each of the arrays and then sum all outputs from all arrays\n",
    "                    loss = loss + 1e-5 * sum([(wei ** 2).sum() for wei in self.model.parameters()])\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ### Compute epoch_loss and epoch_acc\n",
    "                epoch_loss += loss.detach().item() * batch_importance\n",
    "                acc = sum(torch.argmax(batch_predictions, dim = 1) == batch_output) / len(batch_output)\n",
    "                epoch_acc += acc.detach().item() * batch_importance\n",
    "            val_loss, val_acc = self.evaluate(val_inputs, val_outputs, print_acc=False)\n",
    "            if n_epoch % 10 ==0 and not silent: \n",
    "                print(\"Epoch %d/%d - Loss: %.3f - Acc: %.3f\" % (n_epoch + 1, self.epoch, epoch_loss, epoch_acc))\n",
    "                print(\"              Val_loss: %.3f - Val_acc: %.3f\" % (val_loss, val_acc))\n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "            if tens_logger is not None:\n",
    "                tens_logger.add_scalar(\"losses\", epoch_loss, n_epoch + 1)\n",
    "                tens_logger.add_scalar(\"accuracies\", epoch_acc, n_epoch + 1)\n",
    "                tens_logger.add_scalar(\"val_losses\", val_loss, n_epoch + 1)\n",
    "                tens_logger.add_scalar(\"val_accuracies\", val_acc, n_epoch + 1)\n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"losses\": losses, \"accuracies\": accuracies, \"val_losses\": val_losses, \"val_accuracies\": val_accuracies,\n",
    "               \"model\": self.model}\n",
    "        \n",
    "    def evaluate(self, inputs, outputs, print_acc=True):\n",
    "        \"\"\" evaluate model on provided input and output\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        print_acc: bool\n",
    "        \n",
    "        @return\n",
    "        losses: float\n",
    "        acc: float\n",
    "        \"\"\"\n",
    "        tensor_inp = torch.tensor(inputs, dtype = torch.float)\n",
    "        tensor_out = torch.tensor(outputs, dtype = torch.int64)\n",
    "        \n",
    "        preds = self.model(tensor_inp)\n",
    "        losses = nn.CrossEntropyLoss()(preds, tensor_out)\n",
    "        losses = losses.item()\n",
    "        acc = (sum(torch.argmax(preds, axis= 1) == tensor_out) / len(tensor_out)).item()\n",
    "        if print_acc:\n",
    "            print(\"Accuracy: %.3f\" % acc)\n",
    "        return losses, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749c76f",
   "metadata": {},
   "source": [
    "The fact that I am getting comparable accuracy with 20 * 128 reduction in number of epochs is insane, time per epoch has increased tho to 5 seconds so takes 5 seconds to run through all the batches in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7beaa497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 32, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion of test dataset to right shape\n",
    "temp = lambda x: x.reshape(x.shape[0], 1 , x.shape[1], x.shape[2])\n",
    "reshaped_test_x = temp(test_X)\n",
    "reshaped_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f2c3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "cnn_logger = SummaryWriter(log_dir = 'CNN_MNIST')\n",
    "cnn_logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e578cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea67e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40200, 32, 32)\n",
      "(19800, 32, 32)\n",
      "(40200,)\n",
      "(19800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a59137",
   "metadata": {},
   "source": [
    "## Running Model With Both Batch Normalization & Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4801f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▍                                          | 1/30 [00:12<05:48, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.278 - Acc: 0.922\n",
      "              Val_loss: 0.105 - Val_acc: 0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████▊                           | 11/30 [02:14<03:53, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Loss: 0.020 - Acc: 0.995\n",
      "              Val_loss: 0.039 - Val_acc: 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████             | 21/30 [04:18<01:52, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Loss: 0.009 - Acc: 0.999\n",
      "              Val_loss: 0.043 - Val_acc: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'train'  took: 371.2909 sec\n",
      "Test Accuracy: 0.9909999966621399\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "mlp_trainer = Trainer(cnn, 'adam', 1e-3, 30, 128)\n",
    "train_val_dict = mlp_trainer.train(X_train, y_train, X_val,y_val, early_stop= True, l2= True, tens_logger= cnn_logger)\n",
    "test_acc = mlp_trainer.evaluate(reshaped_test_x, test_y, print_acc= False)[1]\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6c054",
   "metadata": {},
   "source": [
    "## Running Model Without Batch Normalization But Have Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93941133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(in_channels= 1, out_channels= 6,kernel_size = 5, stride = 1, padding= 0),\n",
    "                                   nn.Conv2d(in_channels= 6, out_channels= 12,kernel_size = 5, stride = 1, padding= 0)])\n",
    "        self.fc = nn.ModuleList([nn.Linear(300, 300), nn.Linear(300, 10) ])\n",
    "        #self.bn = [nn.BatchNorm2d(6),nn.BatchNorm2d(12)]\n",
    "        self.pooling = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(2):\n",
    "            x = self.pooling(self.activation((self.conv[i](x))))\n",
    "        x = nn.Flatten() (x)\n",
    "        res2 = x\n",
    "        y = self.activation(self.fc[0] (x))\n",
    "        y = y + res2\n",
    "        y = self.fc[1](y)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "238a9fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-1                       [-1, 6, 28, 28]           156\n",
      "├─ReLU: 1-1                              [-1, 6, 28, 28]           --\n",
      "├─MaxPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-2                       [-1, 12, 10, 10]          1,812\n",
      "├─ReLU: 1-3                              [-1, 12, 10, 10]          --\n",
      "├─MaxPool2d: 1-4                         [-1, 12, 5, 5]            --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-3                       [-1, 300]                 90,300\n",
      "├─ReLU: 1-5                              [-1, 300]                 --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-4                       [-1, 10]                  3,010\n",
      "==========================================================================================\n",
      "Total params: 95,278\n",
      "Trainable params: 95,278\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.39\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 0.41\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "# Input is the (number of channels, image height, image width) if input is 2d, essentially the 2 dimensions of it\n",
    "s = summary(model, (1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcdca105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▍                                          | 1/30 [00:10<05:17, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 1.372 - Acc: 0.848\n",
      "              Val_loss: 0.174 - Val_acc: 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████▊                           | 11/30 [02:04<03:36, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Loss: 0.026 - Acc: 0.992\n",
      "              Val_loss: 0.102 - Val_acc: 0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████             | 21/30 [03:57<01:41, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Loss: 0.018 - Acc: 0.995\n",
      "              Val_loss: 0.105 - Val_acc: 0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'train'  took: 338.3172 sec\n",
      "Test Accuracy: 0.9855999946594238\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "mlp_trainer = Trainer(cnn, 'adam', 1e-3, 30, 128)\n",
    "train_val_dict = mlp_trainer.train(X_train, y_train, X_val,y_val, early_stop= True, l2= True, tens_logger= cnn_logger)\n",
    "test_acc = mlp_trainer.evaluate(reshaped_test_x, test_y, print_acc= False)[1]\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14eddc",
   "metadata": {},
   "source": [
    "### Getting better test accuracy when running the model with Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ab51f",
   "metadata": {},
   "source": [
    "## Q1B: Running Model with Batch Norm but NO skip connection at 10 epochs & LR of 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97f4faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(in_channels= 1, out_channels= 6,kernel_size = 5, stride = 1, padding= 0),\n",
    "                                   nn.Conv2d(in_channels= 6, out_channels= 12,kernel_size = 5, stride = 1, padding= 0)])\n",
    "        self.fc = nn.ModuleList([nn.Linear(300, 300), nn.Linear(300, 10) ])\n",
    "        self.bn = [nn.BatchNorm2d(6),nn.BatchNorm2d(12)]\n",
    "        self.pooling = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(2):\n",
    "            x = self.pooling(self.activation(self.bn[i](self.conv[i](x))))\n",
    "        x = nn.Flatten() (x)\n",
    "        x = self.activation(self.fc[0] (x))\n",
    "        x = self.fc[1](x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f165eb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-1                       [-1, 6, 28, 28]           156\n",
      "├─ReLU: 1-1                              [-1, 6, 28, 28]           --\n",
      "├─MaxPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-2                       [-1, 12, 10, 10]          1,812\n",
      "├─ReLU: 1-3                              [-1, 12, 10, 10]          --\n",
      "├─MaxPool2d: 1-4                         [-1, 12, 5, 5]            --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-3                       [-1, 300]                 90,300\n",
      "├─ReLU: 1-5                              [-1, 300]                 --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-4                       [-1, 10]                  3,010\n",
      "==========================================================================================\n",
      "Total params: 95,278\n",
      "Trainable params: 95,278\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.39\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 0.41\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "# Input is the (number of channels, image height, image width) if input is 2d, essentially the 2 dimensions of it\n",
    "s = summary(model, (1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00fcdecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:12<01:51, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.179 - Acc: 0.945\n",
      "              Val_loss: 0.091 - Val_acc: 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'train'  took: 130.5418 sec\n",
      "Test Accuracy: 0.9864000082015991\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "mlp_trainer = Trainer(cnn, 'adam', 5e-3, 10, 128)\n",
    "train_val_dict = mlp_trainer.train(X_train, y_train, X_val,y_val, early_stop= True, l2= True, tens_logger= cnn_logger)\n",
    "test_acc = mlp_trainer.evaluate(reshaped_test_x, test_y, print_acc= False)[1]\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804f799",
   "metadata": {},
   "source": [
    "## Running the Model with Both Skip Connection & Batch Norm at 10 epochs and LR of 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d426b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(in_channels= 1, out_channels= 6,kernel_size = 5, stride = 1, padding= 0),\n",
    "                                   nn.Conv2d(in_channels= 6, out_channels= 12,kernel_size = 5, stride = 1, padding= 0)])\n",
    "        self.fc = nn.ModuleList([nn.Linear(300, 300), nn.Linear(300, 10) ])\n",
    "        self.bn = [nn.BatchNorm2d(6),nn.BatchNorm2d(12)]\n",
    "        self.pooling = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(2):\n",
    "            x = self.pooling(self.activation(self.bn[i](self.conv[i](x))))\n",
    "        x = nn.Flatten() (x)\n",
    "        res2 = x\n",
    "        y = self.activation(self.fc[0] (x))\n",
    "        y = y + res2\n",
    "        y = self.fc[1](y)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aa05b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-1                       [-1, 6, 28, 28]           156\n",
      "├─ReLU: 1-1                              [-1, 6, 28, 28]           --\n",
      "├─MaxPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Conv2d: 2-2                       [-1, 12, 10, 10]          1,812\n",
      "├─ReLU: 1-3                              [-1, 12, 10, 10]          --\n",
      "├─MaxPool2d: 1-4                         [-1, 12, 5, 5]            --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-3                       [-1, 300]                 90,300\n",
      "├─ReLU: 1-5                              [-1, 300]                 --\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-4                       [-1, 10]                  3,010\n",
      "==========================================================================================\n",
      "Total params: 95,278\n",
      "Trainable params: 95,278\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.39\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 0.41\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "# Input is the (number of channels, image height, image width) if input is 2d, essentially the 2 dimensions of it\n",
    "s = summary(model, (1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5548d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:12<01:52, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.183 - Acc: 0.946\n",
      "              Val_loss: 0.062 - Val_acc: 0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'train'  took: 130.5511 sec\n",
      "Test Accuracy: 0.9879999756813049\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "mlp_trainer = Trainer(cnn, 'adam', 5e-3, 10, 128)\n",
    "train_val_dict = mlp_trainer.train(X_train, y_train, X_val,y_val, early_stop= True, l2= True, tens_logger= cnn_logger)\n",
    "test_acc = mlp_trainer.evaluate(reshaped_test_x, test_y, print_acc= False)[1]\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81148236",
   "metadata": {},
   "source": [
    "### Getting slightly better test accuracy with the skip connection, and around the same training time for both versions of this model (without and with skip connection for 10 epochs and LR of 5e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_life",
   "language": "python",
   "name": "ml_life"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
