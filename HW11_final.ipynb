{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "Some functions that might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles = pickle.load(open(\"ani_smiles.pkl\", \"rb\"))\n",
    "print(len(smiles))\n",
    "type(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_char = 'EOS'\n",
    "SOS_char = 'SOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SOS',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  'C',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  'EOS'],\n",
       " ['SOS', '[', 'H', ']', 'N', '(', '[', 'H', ']', ')', '[', 'H', ']', 'EOS']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding SOS_char and EOS_char\n",
    "smiles_copy = smiles.copy()\n",
    "for smile_str in smiles_copy:\n",
    "    smile_str.append(EOS_char)\n",
    "    smile_str.insert(0, SOS_char)\n",
    "smiles_copy[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#', '(', ')', '1', '2', '=', 'C', 'EOS', 'H', 'N', 'O', 'SOS',\n",
       "       '[', ']', 'c', 'n', 'o'], dtype='<U3')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return array of unique entries in all SMILES strings\n",
    "temp = [list(set(entry)) for entry in smiles_copy] # Return list of lists of unique entries in each SMILES string\n",
    "temps = np.array(sum(temp, [])) # Return a collapsed 1-D array of temp \n",
    "unique_chars = np.unique(temps)\n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matching expected\n",
    "len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(unique_chars.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['#', '(', ')', '1', '2', '=', 'C', 'EOS', 'H', 'N', 'O', 'SOS',\n",
       "        '[', ']', 'c', 'n', 'o'], dtype='<U3')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19x17 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform(np.array(smiles[0]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def batches_gen(smiles, batchsize, encoder):\n",
    "    '''Create a generator that returns batches of size (batch_size,seq_leng,nchars) from smiles, \n",
    "    where seq_leng is the length of the longest smiles string and nchar is the length of one-hot encoded characters (17)\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       smiles: python list(nsmiles,nchar) smiles array shape you want to make batches from\n",
    "       batchsize: Batch size, the number of sequences per batch\n",
    "       encoder: one hot encoder\n",
    "\n",
    "    '''\n",
    "    arr=[torch.tensor(np.array(encoder.transform(np.array(s).reshape(-1,1)).toarray()),dtype=torch.float) for s in smiles] \n",
    "        #size (nsmiles,seq_length(variable),nchars)\n",
    "        \n",
    "    # The features\n",
    "    X = [s[:-1,:] for s in arr]\n",
    "    # The targets, shifted by one\n",
    "    y = [s[1:,:] for s in arr]\n",
    "    # pad sequence so that all smiles are the same length\n",
    "    X = nn.utils.rnn.pad_sequence(X,batch_first=True)\n",
    "    y = nn.utils.rnn.pad_sequence(y,batch_first=True)\n",
    "\n",
    "    \n",
    "    for i in range(len(arr)//batchsize):\n",
    "        yield X[i*batchsize:(i+1)*batchsize],y[i*batchsize:(i+1)*batchsize]\n",
    "        \n",
    "    #drop last batch that is not the same size due to hidden state constraint\n",
    "\n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_prop80 = int(len(smiles_copy) * 0.8)\n",
    "smiles_prop80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = batches_gen(smiles= smiles_copy, batchsize= 1, encoder= enc)\n",
    "val_set = batches_gen(smiles = smiles_copy[smiles_prop80 :], batchsize= 1, encoder= enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_layers = 1\n",
    "        self.n_hidden = 32\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size= 17, # Input Size \n",
    "            hidden_size=32,     # rnn hidden unit\n",
    "            num_layers=1,       # number of rnn layer\n",
    "            batch_first=True,)\n",
    "        self.out = nn.Linear(32, 17)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        r_out, h_state = self.lstm(x, h_state)\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "    \n",
    "    def init_state(self, batchsize):\n",
    "        return (torch.zeros(self.n_layers, batchsize, self.n_hidden), #hidden state\n",
    "                torch.zeros(self.n_layers, batchsize, self.n_hidden)) #cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()\n",
    "LR = 0.02           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x.reshape(x.shape[0], -1)):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = Adam(model.parameters(), learning_rate)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "\n",
    "    def train(self,early_stop=False,l2=False,silent=False):\n",
    "        \"\"\" train self.model with specified arguments\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_inputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \n",
    "        @return\n",
    "        a dictionary of arrays with train and val losses and accuracies\n",
    "        \"\"\"\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        h_state,c_state = self.model.init_state(1)\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        loss_func = nn.MSELoss()\n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            train_set = batches_gen(smiles= smiles_copy, batchsize= 1, encoder= enc)\n",
    "            for x,y in train_set:\n",
    "                prediction, (h_state, c_state) = lstm(x, (h_state,c_state))   # rnn output\n",
    "                h_state = h_state.detach()\n",
    "                c_state = c_state.detach()\n",
    "                loss = loss_func(prediction, y)         # calculate loss\n",
    "                self.optimizer.zero_grad()                   # clear gradients for this training step\n",
    "                loss.backward()                         # backpropagation, compute gradients\n",
    "                self.optimizer.step()                        # apply gradients\n",
    "                \n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"model\": self.model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer = Trainer(lstm, \"adam\", 0.02, 17, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "trained_lstm = lstm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(17, 32, batch_first=True)\n",
       "  (out): Linear(in_features=32, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lstm['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a method to generate the next character\n",
    "def predict(net, inputs, h, top_k=None):\n",
    "        ''' Given a onehot encoded character, predict the next character.\n",
    "            Returns the predicted onehot encoded character and the hidden state.\n",
    "        Arguments:\n",
    "            net: the lstm model\n",
    "            inputs: input to the lstm model. shape (batch, time_step/length_of_smiles, input_size) with batchsize of 1\n",
    "            h: hidden state (h,c)\n",
    "            top_k: int. sample from top k possible characters\n",
    "            \n",
    "        '''\n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "        # get the character probabilities\n",
    "        p = out.data\n",
    "\n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(17) #index to choose from\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        # return the onehot encoded value of the predicted char and the hidden state\n",
    "        output = np.zeros(inputs.detach().numpy().shape)\n",
    "        output[:,:,char] = 1\n",
    "        output = torch.tensor(output,dtype=torch.float)\n",
    "        return output, h\n",
    "\n",
    "# Declaring a method to generate new text\n",
    "def sample(net, encoder, prime=['SOS'], top_k=None):\n",
    "    \"\"\"generate a smiles string starting from prime. I use 'SOS' (start of string) and 'EOS'(end of string). \n",
    "    You may need to change this based on your starting and ending character.\n",
    "\n",
    "    \"\"\"\n",
    "    net.eval() # eval mode\n",
    "    # get initial hidden state with batchsize 1\n",
    "    h = net.init_state(1)\n",
    "    # First off, run through the prime characters\n",
    "    chars=[]\n",
    "    for ch in prime:\n",
    "        ch = encoder.transform(np.array([ch]).reshape(-1, 1)).toarray() #(1,17)\n",
    "        ch = torch.tensor(ch,dtype=torch.float).reshape(1,1,17)\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "    chars.append(char)\n",
    "    end  = encoder.transform(np.array(['EOS']).reshape(-1, 1)).toarray()\n",
    "    end = torch.tensor(end,dtype=torch.float).reshape(1,1,17)\n",
    "\n",
    "    # Now pass in the previous character and get a new one\n",
    "    while not torch.all(end.eq(chars[-1])):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "    chars =[c.detach().numpy() for c in chars]\n",
    "    chars = np.array(chars).reshape(-1,17)\n",
    "    chars = encoder.inverse_transform(chars).reshape(-1)\n",
    "    return ''.join(chars[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A website to check if your smiles is valid: https://chemwriter.com/smiles/ It'll show you a figure for the valid string!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out LSTM Generated SMILE Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H]ON(=O)C([H])C([H])([H])=N==CC([H])([H])C([H])=#NC([H])([H])[H]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a valid SMILE string\n",
    "sample(trained_lstm['model'], enc, prime = ['SOS'], top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H](C(C([H])([H])[H])[H])C([H])([H])['"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a valid SMILE string\n",
    "sample(trained_lstm['model'], enc, prime = ['SOS'], top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H]C(=NOC([#N'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a valid SMILE string\n",
    "sample(trained_lstm['model'], enc, prime = ['SOS'], top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H]C=NN([H])OC([H])C(#C([H]C1[=NC([H])([HC([H])([H])[H])[H]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a valid SMILE string\n",
    "sample(trained_lstm['model'], enc, prime = ['SOS'], top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[HC(=#CC([H])#CC([H]#C(([#CC([H])([H])[H=C1[H]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a valid SMILE string\n",
    "sample(trained_lstm['model'], enc, prime = ['SOS'], top_k=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
